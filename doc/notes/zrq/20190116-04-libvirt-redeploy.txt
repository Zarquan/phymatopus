#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2019, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    TODO Install Kafka ...
    doc/notes/zrq/20181210-02-trop-transfer.txt
    doc/notes/zrq/20181210-03-trop-transfer.txt
    doc/notes/zrq/20181211-02-trop-transfer.txt

    Fixed
    doc/notes/zrq/20181211-03-trop-transfer.txt

# -----------------------------------------------------
# Our Kafka nodes.
#[user@trop03]

    kfnames=(
        Stedigo
        Angece
        Edwalafia
        Onoza
        )


# -----------------------------------------------------
# Create our Kafka nodes.
# TODO scriptable createvm
#[user@trop03]

    createvm

--START--
INFO : Node name [Stedigo]
INFO : Base name [fedora-28-8G-docker-base-20181016.qcow]
INFO : Base path [/var/lib/libvirt/images/base/fedora-28-8G-docker-base-20181016.qcow]
INFO : Disc name [Stedigo.qcow]
INFO : Disc size [8GiB]

INFO : node [9]
INFO : MAC  [52:54:0:0:D2:8A]
INFO : IPv4 [192.168.210.138]
INFO : MAC  [52:54:0:0:D2:AA]
INFO : IPv4 [192.168.210.170]
--END--


    createvm

--START--
INFO : Node name [Angece]
INFO : Base name [fedora-28-8G-docker-base-20181016.qcow]
INFO : Base path [/var/lib/libvirt/images/base/fedora-28-8G-docker-base-20181016.qcow]
INFO : Disc name [Angece.qcow]
INFO : Disc size [8GiB]

INFO : node [10]
INFO : MAC  [52:54:0:0:D2:8B]
INFO : IPv4 [192.168.210.139]
INFO : MAC  [52:54:0:0:D2:AB]
INFO : IPv4 [192.168.210.171]
--END--


    createvm

--START--
INFO : Node name [Edwalafia]
INFO : Base name [fedora-28-8G-docker-base-20181016.qcow]
INFO : Base path [/var/lib/libvirt/images/base/fedora-28-8G-docker-base-20181016.qcow]
INFO : Disc name [Edwalafia.qcow]
INFO : Disc size [8GiB]

INFO : node [11]
INFO : MAC  [52:54:0:0:D2:8C]
INFO : IPv4 [192.168.210.140]
INFO : MAC  [52:54:0:0:D2:AC]
INFO : IPv4 [192.168.210.172]
--END--


    createvm

--START--
INFO : Node name [Onoza]
INFO : Base name [fedora-28-8G-docker-base-20181016.qcow]
INFO : Base path [/var/lib/libvirt/images/base/fedora-28-8G-docker-base-20181016.qcow]
INFO : Disc name [Onoza.qcow]
INFO : Disc size [8GiB]

INFO : node [12]
INFO : MAC  [52:54:0:0:D2:8D]
INFO : IPv4 [192.168.210.141]
INFO : MAC  [52:54:0:0:D2:AD]
INFO : IPv4 [192.168.210.173]
--END--


# -----------------------------------------------------
# Attach the existing data volumes to each Kafka node.
#[user@trop03]

    while read line
    do
        vmname=$(echo "${line}" | awk '{print $1}')
        target=$(echo "${line}" | awk '{print $2}')
        source=$(echo "${line}" | awk '{print $3}')
        echo "[${vmname}][${target}][${source}]"

        virsh \
            --connect "${connection:?}" \
            attach-disk \
                ${vmname:?}   \
                ${source:?}  \
                ${target:?}   \
                --subdriver qcow2 \
                --driver qemu  \
                --config

    done < "${tmpfile}"

--START--
[Stedigo][vdc][/data1/libvirt/images/data1/Stedigo-data1-01.qcow]
Disk attached successfully

[Stedigo][vdd][/data2/libvirt/images/data2/Stedigo-data2-01.qcow]
Disk attached successfully

[Stedigo][vde][/data1/libvirt/images/data1/Stedigo-data1-02.qcow]
Disk attached successfully

[Stedigo][vdf][/data2/libvirt/images/data2/Stedigo-data2-02.qcow]
Disk attached successfully

[Angece][vdc][/data1/libvirt/images/data1/Angece-data1-01.qcow]
Disk attached successfully

[Angece][vdd][/data2/libvirt/images/data2/Angece-data2-01.qcow]
Disk attached successfully

[Angece][vde][/data1/libvirt/images/data1/Angece-data1-02.qcow]
Disk attached successfully

[Angece][vdf][/data2/libvirt/images/data2/Angece-data2-02.qcow]
Disk attached successfully

[Edwalafia][vdc][/data1/libvirt/images/data1/Edwalafia-data1-01.qcow]
Disk attached successfully

[Edwalafia][vdd][/data2/libvirt/images/data2/Edwalafia-data2-01.qcow]
Disk attached successfully

[Edwalafia][vde][/data1/libvirt/images/data1/Edwalafia-data1-02.qcow]
Disk attached successfully

[Edwalafia][vdf][/data2/libvirt/images/data2/Edwalafia-data2-02.qcow]
Disk attached successfully

[Onoza][vdc][/data1/libvirt/images/data1/Onoza-data1-01.qcow]
Disk attached successfully

[Onoza][vdd][/data2/libvirt/images/data2/Onoza-data2-01.qcow]
Disk attached successfully

[Onoza][vde][/data1/libvirt/images/data1/Onoza-data1-02.qcow]
Disk attached successfully

[Onoza][vdf][/data2/libvirt/images/data2/Onoza-data2-02.qcow]
Disk attached successfully
--END--


# -----------------------------------------------------
# Create extra data volumes for each Kafka node.
#[user@trop03]

    source "${HOME}/libvirt.settings"

    volnum=03
    volsize=256G

    unset volpools
    declare -a volpools=(
        data1
        data2
        )

    unset targets
    declare -A targets=(
        [data1]=vdg
        [data2]=vdh
        )

    for vmname in ${kfnames[@]}
        do
            echo "---- ----"
            echo "vmname [${vmname}]"
            for volpool in ${volpools[@]}
                do
                    volname=${vmname}-${volpool}-${volnum}.qcow
                    target=${targets[${volpool}]}
                    echo "create [${vmname}][${target}][${volname}]"

                    virsh \
                        --connect "${connection:?}" \
                        vol-create-as \
                            "${volpool}" \
                            "${volname}" \
                            "${volsize}" \
                            --allocation 0 \
                            --format qcow2

                    virsh \
                        --connect "${connection:?}" \
                        vol-info \
                            --pool "${volpool:?}" \
                            "${volname:?}"

                    volpath=$(
                        virsh \
                            --connect "${connection:?}" \
                            vol-path \
                                --pool "${volpool:?}" \
                                "${volname:?}"
                        )

                    echo "attach [${vmname}][${target}][${volpath}]"

                    virsh \
                        --connect "${connection:?}" \
                        attach-disk \
                            "${vmname:?}"  \
                            "${volpath:?}" \
                            "${target:?}"  \
                            --subdriver qcow2 \
                            --driver qemu  \
                            --config
                done
        done

--START--
---- ----
vmname [Stedigo]
create [Stedigo][vdg][Stedigo-data1-03.qcow]
Vol Stedigo-data1-03.qcow created

Name:           Stedigo-data1-03.qcow
Type:           file
Capacity:       256.00 GiB
Allocation:     196.00 KiB

attach [Stedigo][vdg][/data1/libvirt/images/data1/Stedigo-data1-03.qcow]
Disk attached successfully

create [Stedigo][vdh][Stedigo-data2-03.qcow]
Vol Stedigo-data2-03.qcow created

Name:           Stedigo-data2-03.qcow
Type:           file
Capacity:       256.00 GiB
Allocation:     196.00 KiB

attach [Stedigo][vdh][/data2/libvirt/images/data2/Stedigo-data2-03.qcow]
Disk attached successfully

---- ----
vmname [Angece]
create [Angece][vdg][Angece-data1-03.qcow]
Vol Angece-data1-03.qcow created

Name:           Angece-data1-03.qcow
Type:           file
Capacity:       256.00 GiB
Allocation:     196.00 KiB

attach [Angece][vdg][/data1/libvirt/images/data1/Angece-data1-03.qcow]
Disk attached successfully

create [Angece][vdh][Angece-data2-03.qcow]
Vol Angece-data2-03.qcow created

Name:           Angece-data2-03.qcow
Type:           file
Capacity:       256.00 GiB
Allocation:     196.00 KiB

attach [Angece][vdh][/data2/libvirt/images/data2/Angece-data2-03.qcow]
Disk attached successfully

---- ----
vmname [Edwalafia]
create [Edwalafia][vdg][Edwalafia-data1-03.qcow]
Vol Edwalafia-data1-03.qcow created

Name:           Edwalafia-data1-03.qcow
Type:           file
Capacity:       256.00 GiB
Allocation:     196.00 KiB

attach [Edwalafia][vdg][/data1/libvirt/images/data1/Edwalafia-data1-03.qcow]
Disk attached successfully

create [Edwalafia][vdh][Edwalafia-data2-03.qcow]
Vol Edwalafia-data2-03.qcow created

Name:           Edwalafia-data2-03.qcow
Type:           file
Capacity:       256.00 GiB
Allocation:     196.00 KiB

attach [Edwalafia][vdh][/data2/libvirt/images/data2/Edwalafia-data2-03.qcow]
Disk attached successfully

---- ----
vmname [Onoza]
create [Onoza][vdg][Onoza-data1-03.qcow]
Vol Onoza-data1-03.qcow created

Name:           Onoza-data1-03.qcow
Type:           file
Capacity:       256.00 GiB
Allocation:     196.00 KiB

attach [Onoza][vdg][/data1/libvirt/images/data1/Onoza-data1-03.qcow]
Disk attached successfully

create [Onoza][vdh][Onoza-data2-03.qcow]
Vol Onoza-data2-03.qcow created

Name:           Onoza-data2-03.qcow
Type:           file
Capacity:       256.00 GiB
Allocation:     196.00 KiB

attach [Onoza][vdh][/data2/libvirt/images/data2/Onoza-data2-03.qcow]
Disk attached successfully
--END--


# -----------------------------------------------------
# Restart the kafka nodes.

    for vmname in ${kfnames[@]}
        do
            virsh \
                --connect "${connection:?}" \
                shutdown \
                    "${vmname}"
        done

    sleep 60

    for vmname in ${kfnames[@]}
        do
            virsh \
                --connect "${connection:?}" \
                start \
                    "${vmname}"
        done

--START--
Domain Stedigo is being shutdown
Domain Angece is being shutdown
Domain Edwalafia is being shutdown
Domain Onoza is being shutdown
--END--

--START--
Domain Stedigo started
Domain Angece started
Domain Edwalafia started
Domain Onoza started
--END--


# -----------------------------------------------------
# List the volumes on each Kafka node.
# http://xmlstar.sourceforge.net/doc/UG/ch04.html
# https://sourceforge.net/p/xmlstar/discussion/226076/thread/d5eca10f/#56b4
#[user@trop03]

    for vmname in ${kfnames[@]}
        do
            echo "$(printf '+ %56s +' '')" | sed 's/ /-/g'
            echo "$(printf '| %-56s |' 'Node ['${vmname:?}']')"
            virsh \
                --connect "${connection:?}" \
                dumpxml \
                    "${vmname}" \
            | xmlstarlet \
                select \
                    --text \
                    --template \
                        --match '//disk' \
                        --sort  'A:T:-' 'target/@dev' \
                        --value-of "concat('| ', target/@dev, ' | ', str:align(source/@file, str:padding(50, ' '), 'left'), ' |')" \
                        --nl
        done \
    ; echo "$(printf '+ %56s +' '')" | sed 's/ /-/g'

--START--
+----------------------------------------------------------+
| Node [Stedigo]                                           |
| vda | /var/lib/libvirt/images/live/Stedigo.qcow          |
| vdb | /var/lib/libvirt/images/init/Stedigo.iso           |
| vdc | /data1/libvirt/images/data1/Stedigo-data1-01.qcow  |
| vdd | /data2/libvirt/images/data2/Stedigo-data2-01.qcow  |
| vde | /data1/libvirt/images/data1/Stedigo-data1-02.qcow  |
| vdf | /data2/libvirt/images/data2/Stedigo-data2-02.qcow  |
| vdg | /data1/libvirt/images/data1/Stedigo-data1-03.qcow  |
| vdh | /data2/libvirt/images/data2/Stedigo-data2-03.qcow  |
+----------------------------------------------------------+
| Node [Angece]                                            |
| vda | /var/lib/libvirt/images/live/Angece.qcow           |
| vdb | /var/lib/libvirt/images/init/Angece.iso            |
| vdc | /data1/libvirt/images/data1/Angece-data1-01.qcow   |
| vdd | /data2/libvirt/images/data2/Angece-data2-01.qcow   |
| vde | /data1/libvirt/images/data1/Angece-data1-02.qcow   |
| vdf | /data2/libvirt/images/data2/Angece-data2-02.qcow   |
| vdg | /data1/libvirt/images/data1/Angece-data1-03.qcow   |
| vdh | /data2/libvirt/images/data2/Angece-data2-03.qcow   |
+----------------------------------------------------------+
| Node [Edwalafia]                                         |
| vda | /var/lib/libvirt/images/live/Edwalafia.qcow        |
| vdb | /var/lib/libvirt/images/init/Edwalafia.iso         |
| vdc | /data1/libvirt/images/data1/Edwalafia-data1-01.qco |
| vdd | /data2/libvirt/images/data2/Edwalafia-data2-01.qco |
| vde | /data1/libvirt/images/data1/Edwalafia-data1-02.qco |
| vdf | /data2/libvirt/images/data2/Edwalafia-data2-02.qco |
| vdg | /data1/libvirt/images/data1/Edwalafia-data1-03.qco |
| vdh | /data2/libvirt/images/data2/Edwalafia-data2-03.qco |
+----------------------------------------------------------+
| Node [Onoza]                                             |
| vda | /var/lib/libvirt/images/live/Onoza.qcow            |
| vdb | /var/lib/libvirt/images/init/Onoza.iso             |
| vdc | /data1/libvirt/images/data1/Onoza-data1-01.qcow    |
| vdd | /data2/libvirt/images/data2/Onoza-data2-01.qcow    |
| vde | /data1/libvirt/images/data1/Onoza-data1-02.qcow    |
| vdf | /data2/libvirt/images/data2/Onoza-data2-02.qcow    |
| vdg | /data1/libvirt/images/data1/Onoza-data1-03.qcow    |
| vdh | /data2/libvirt/images/data2/Onoza-data2-03.qcow    |
+----------------------------------------------------------+
--END--


# -----------------------------------------------------
# Deploy and run the netfix script on each node.
# doc/notes/zrq/20190116-03-libvirt-redeploy.txt
#[user@trop03]

    source "${HOME}/ssh-options"

    for vmname in ${kfnames[@]}
        do
            echo "---- ----"
            echo "Node [${vmname:?}]"
            scp \
                ${scpopts[*]} \
                /tmp/netfix.sh \
                ${sshuser:?}@${vmname:?}:/tmp/netfix.sh

            ssh \
                ${sshopts[*]} \
                ${sshuser:?}@${vmname:?} \
                    "
                    date
                    hostname
                    chmod a+x '/tmp/netfix.sh'
                    sudo -s   '/tmp/netfix.sh'
                    "
        done

--START--
---- ----
Node [Stedigo]
netfix.sh   100%  966     1.6MB/s   00:00
Thu 17 Jan 23:50:12 GMT 2019
Stedigo
/etc/sysconfig /home/Stevedore
/etc/sysconfig/network-scripts /etc/sysconfig /home/Stevedore
/etc/sysconfig /home/Stevedore
---- ----
Node [Angece]
netfix.sh   100%  966     1.7MB/s   00:00
Thu 17 Jan 23:50:14 GMT 2019
Angece
/etc/sysconfig /home/Stevedore
/etc/sysconfig/network-scripts /etc/sysconfig /home/Stevedore
/etc/sysconfig /home/Stevedore
---- ----
Node [Edwalafia]
netfix.sh   100%  966     1.0MB/s   00:00
Thu 17 Jan 23:50:15 GMT 2019
Edwalafia
/etc/sysconfig /home/Stevedore
/etc/sysconfig/network-scripts /etc/sysconfig /home/Stevedore
/etc/sysconfig /home/Stevedore
---- ----
Node [Onoza]
netfix.sh   100%  966     1.9MB/s   00:00
Thu 17 Jan 23:50:16 GMT 2019
Onoza
/etc/sysconfig /home/Stevedore
/etc/sysconfig/network-scripts /etc/sysconfig /home/Stevedore
/etc/sysconfig /home/Stevedore
--END--


# -----------------------------------------------------
# Shutdown and restart each of our Kafka nodes.
#[user@trop03]

    source "${HOME}/libvirt.settings"

    for vmname in ${kfnames[@]}
        do
            virsh \
                --connect ${connection:?} \
                    shutdown \
                    ${vmname:?}
        done

    sleep 30

    for vmname in ${kfnames[@]}
        do
            virsh \
                --connect ${connection:?} \
                    start \
                    ${vmname:?}
        done

--START--
Domain Stedigo is being shutdown
Domain Angece is being shutdown
Domain Edwalafia is being shutdown
Domain Onoza is being shutdown
--END--

--START--
Domain Stedigo started
Domain Angece started
Domain Edwalafia started
Domain Onoza started
--END--


# -----------------------------------------------------
# Check the number of cores on our Kafka nodes.
#[user@trop03]

    for vmname in ${kfnames[@]}
        do
            virsh \
                --quiet \
                --connect ${connection:?} \
                    dumpxml \
                        "${vmname}" \
            | xmlstarlet \
                select \
                    --text \
                    --template \
                        --output "$(printf '%-10s' ${vmname})" \
                        --value-of "domain/vcpu" \
                        --nl
        done

--START--
Stedigo   4
Angece    4
Edwalafia 4
Onoza     4
--END--


# -----------------------------------------------------
# Check the ip routes on each node.
#[user@trop03]

    for vmname in ${kfnames[@]}
        do
            echo ""
            echo "Node [${vmname:?}]"
            ssh \
                ${sshopts[*]} \
                ${sshuser:?}@${vmname:?} \
                "
                hostname
                date
                sudo ip route list
                "
        done

--START--
Node [Stedigo]
Stedigo
Thu 17 Jan 23:52:47 GMT 2019
default via 192.168.210.190 dev ens7 proto dhcp metric 100
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
192.168.210.0/27 via 192.168.210.158 dev ens3 proto static metric 101
192.168.210.64/27 via 192.168.210.158 dev ens3 proto static metric 101
192.168.210.128/27 dev ens3 proto kernel scope link src 192.168.210.138 metric 101
192.168.210.128/27 via 192.168.210.158 dev ens3 proto static metric 101
192.168.210.160/27 dev ens7 proto kernel scope link src 192.168.210.170 metric 100
192.168.210.192/27 via 192.168.210.158 dev ens3 proto static metric 101

Node [Angece]
Angece
Thu 17 Jan 23:52:48 GMT 2019
default via 192.168.210.190 dev ens7 proto dhcp metric 100
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
192.168.210.0/27 via 192.168.210.158 dev ens3 proto static metric 101
192.168.210.64/27 via 192.168.210.158 dev ens3 proto static metric 101
192.168.210.128/27 dev ens3 proto kernel scope link src 192.168.210.139 metric 101
192.168.210.128/27 via 192.168.210.158 dev ens3 proto static metric 101
192.168.210.160/27 dev ens7 proto kernel scope link src 192.168.210.171 metric 100
192.168.210.192/27 via 192.168.210.158 dev ens3 proto static metric 101

Node [Edwalafia]
Edwalafia
Thu 17 Jan 23:52:49 GMT 2019
default via 192.168.210.190 dev ens7 proto dhcp metric 100
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
192.168.210.0/27 via 192.168.210.158 dev ens3 proto static metric 101
192.168.210.64/27 via 192.168.210.158 dev ens3 proto static metric 101
192.168.210.128/27 dev ens3 proto kernel scope link src 192.168.210.140 metric 101
192.168.210.128/27 via 192.168.210.158 dev ens3 proto static metric 101
192.168.210.160/27 dev ens7 proto kernel scope link src 192.168.210.172 metric 100
192.168.210.192/27 via 192.168.210.158 dev ens3 proto static metric 101

Node [Onoza]
Onoza
Thu 17 Jan 23:52:49 GMT 2019
default via 192.168.210.190 dev ens7 proto dhcp metric 100
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
192.168.210.0/27 via 192.168.210.158 dev ens3 proto static metric 101
192.168.210.64/27 via 192.168.210.158 dev ens3 proto static metric 101
192.168.210.128/27 dev ens3 proto kernel scope link src 192.168.210.141 metric 101
192.168.210.128/27 via 192.168.210.158 dev ens3 proto static metric 101
192.168.210.160/27 dev ens7 proto kernel scope link src 192.168.210.173 metric 100
192.168.210.192/27 via 192.168.210.158 dev ens3 proto static metric 101
--END--


#---------------------------------------------------------------------
# Create a script to mount a volume.
#[user@trop03]

cat > '/tmp/volume-mount.sh' << 'EOSH'

echo "---- ----"
echo "hostname [$(hostname)]"
echo "devpath  [${devpath:?}]"
echo "mntpath  [${mntpath:?}]"
echo "---- ----"

#---------------------------------------------------------------------
# Check if the new device has a filesystem.

    sudo btrfs filesystem show "${devpath:?}" > /dev/null 2>&1
    fscheck=$?

#---------------------------------------------------------------------
# Create a filesystem on the new device.

    if [ ${fscheck} == 1 ]
    then
        echo "Creating btrfs filesystem [${devpath:?}]"
        sudo \
            mkfs.btrfs \
                ${devpath:?}
    else
        echo "Found existing filesystem [${devpath:?}]"
    fi

#---------------------------------------------------------------------
# Create our mount point.

    echo "Creating mount point [${mntpath:?}]"
    sudo mkdir -p "${mntpath:?}"
    sudo touch "${mntpath:?}/mount-failed"

#---------------------------------------------------------------------
# Add the volume to our FileSystemTABle.
# https://www.howtoforge.com/reducing-disk-io-by-mounting-partitions-with-noatime

    devuuid=$(
        lsblk --noheadings --output UUID "${devpath:?}"
        )

    echo "Registering filesystem [${mntpath:?}]"
    sudo tee -a /etc/fstab << EOTAB
UUID=${devuuid:?} ${mntpath:?}    btrfs    defaults,noatime    0  0
EOTAB

#---------------------------------------------------------------------
# Mount the new volume.

    sudo \
        mount "${mntpath:?}"

#---------------------------------------------------------------------
# Check the new volume.

    echo "Checking data space [${mntpath:?}]"
    df -h "${mntpath:?}"

EOSH

# -----------------------------------------------------
# Login and mount each of the data volumes.
# https://www.linuxquestions.org/questions/linux-newbie-8/awk-special-character-as-delimiter-4175613862/
#[user@trop03]

    for vmname in ${kfnames[@]}
        do
            echo "-------- -------- -------- --------"
            echo "Node [${vmname:?}]"
            tmpfile=$(mktemp)
            virsh \
                --connect "${connection:?}" \
                dumpxml \
                    "${vmname}" \
            | xmlstarlet \
                select \
                    --text \
                    --template \
                        --match '//disk[starts-with(source/@file, "/data")]' \
                        --sort  'A:T:-' 'target/@dev' \
                        --output "${vmname}," \
                        --value-of "concat(target/@dev, ',', source/@file)" \
                        --nl \
            > "${tmpfile}"

            for line in $(cat "${tmpfile}")
                do
                    volpath=$(echo "${line}" | awk -F ',' '{print $3}')
                    devname=$(echo "${line}" | awk -F ',' '{print $2}')
                    devpath=/dev/${devname}
                    mntpath=$(
                        basename --suffix '.qcow' "${volpath}" \
                        | sed '
                            s/\([[:alnum:]]*\)-\([[:alnum:]]*\)-\([[:alnum:]]*\)/\/\2-\3/
                            '
                        )

                    echo ""
                    echo "[${devpath}][${mntpath}]"

                    ssh ${sshopts[*]} \
                        ${sshuser:?}@${vmname:?} \
                            "
                            export devpath=${devpath:?}
                            export mntpath=${mntpath:?}
                            date
                            hostname
                            echo "[\${devpath}][\${mntpath}]"

                            $(cat /tmp/volume-mount.sh)
                            "
                done
        done

--START--
-------- -------- -------- --------
Node [Stedigo]

[/dev/vdc][/data1-01]
Fri 18 Jan 02:31:48 GMT 2019
Stedigo
[/dev/vdc][/data1-01]
---- ----
hostname [Stedigo]
devpath  [/dev/vdc]
mntpath  [/data1-01]
---- ----
Found existing filesystem [/dev/vdc]
Creating mount point [/data1-01]
Registering filesystem [/data1-01]
UUID=6ddcb70c-2004-4c5b-ad8f-ec89313ca292 /data1-01    btrfs    defaults,noatime    0  0
Checking data space [/data1-01]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdc         32G   17M   30G   1% /data1-01

[/dev/vdd][/data2-01]
Fri 18 Jan 02:31:49 GMT 2019
Stedigo
[/dev/vdd][/data2-01]
---- ----
hostname [Stedigo]
devpath  [/dev/vdd]
mntpath  [/data2-01]
---- ----
Found existing filesystem [/dev/vdd]
Creating mount point [/data2-01]
Registering filesystem [/data2-01]
UUID=b2598b9e-e396-4f77-9230-dc5c9d4091c8 /data2-01    btrfs    defaults,noatime    0  0
Checking data space [/data2-01]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdd         32G   31G  3.2M 100% /data2-01

[/dev/vde][/data1-02]
Fri 18 Jan 02:31:50 GMT 2019
Stedigo
[/dev/vde][/data1-02]
---- ----
hostname [Stedigo]
devpath  [/dev/vde]
mntpath  [/data1-02]
---- ----
Found existing filesystem [/dev/vde]
Creating mount point [/data1-02]
Registering filesystem [/data1-02]
UUID=55e5fcb2-c24a-4a21-8914-7843ac5596a9 /data1-02    btrfs    defaults,noatime    0  0
Checking data space [/data1-02]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vde         64G   50G   13G  80% /data1-02

[/dev/vdf][/data2-02]
Fri 18 Jan 02:31:51 GMT 2019
Stedigo
[/dev/vdf][/data2-02]
---- ----
hostname [Stedigo]
devpath  [/dev/vdf]
mntpath  [/data2-02]
---- ----
Found existing filesystem [/dev/vdf]
Creating mount point [/data2-02]
Registering filesystem [/data2-02]
UUID=b4d70a33-7af9-4762-b2a3-9c0c8c587bf4 /data2-02    btrfs    defaults,noatime    0  0
Checking data space [/data2-02]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdf         64G   47G   16G  75% /data2-02

[/dev/vdg][/data1-03]
Fri 18 Jan 02:31:52 GMT 2019
Stedigo
[/dev/vdg][/data1-03]
---- ----
hostname [Stedigo]
devpath  [/dev/vdg]
mntpath  [/data1-03]
---- ----
Creating btrfs filesystem [/dev/vdg]
btrfs-progs v4.17.1
See http://btrfs.wiki.kernel.org for more information.

Label:              (null)
UUID:               632eb538-ccda-4588-adaa-9d103863d0a4
Node size:          16384
Sector size:        4096
Filesystem size:    256.00GiB
Block group profiles:
  Data:             single            8.00MiB
  Metadata:         DUP               1.00GiB
  System:           DUP               8.00MiB
SSD detected:       no
Incompat features:  extref, skinny-metadata
Number of devices:  1
Devices:
   ID        SIZE  PATH
    1   256.00GiB  /dev/vdg

Creating mount point [/data1-03]
Registering filesystem [/data1-03]
UUID=632eb538-ccda-4588-adaa-9d103863d0a4 /data1-03    btrfs    defaults,noatime    0  0
Checking data space [/data1-03]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdg        256G   17M  254G   1% /data1-03

[/dev/vdh][/data2-03]
Fri 18 Jan 02:31:53 GMT 2019
Stedigo
[/dev/vdh][/data2-03]
---- ----
hostname [Stedigo]
devpath  [/dev/vdh]
mntpath  [/data2-03]
---- ----
Creating btrfs filesystem [/dev/vdh]
btrfs-progs v4.17.1
See http://btrfs.wiki.kernel.org for more information.

Label:              (null)
UUID:               8fddf0be-bd58-4007-9541-8126b0c3d63d
Node size:          16384
Sector size:        4096
Filesystem size:    256.00GiB
Block group profiles:
  Data:             single            8.00MiB
  Metadata:         DUP               1.00GiB
  System:           DUP               8.00MiB
SSD detected:       no
Incompat features:  extref, skinny-metadata
Number of devices:  1
Devices:
   ID        SIZE  PATH
    1   256.00GiB  /dev/vdh

Creating mount point [/data2-03]
Registering filesystem [/data2-03]
UUID=8fddf0be-bd58-4007-9541-8126b0c3d63d /data2-03    btrfs    defaults,noatime    0  0
Checking data space [/data2-03]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdh        256G   17M  254G   1% /data2-03
-------- -------- -------- --------
Node [Angece]

[/dev/vdc][/data1-01]
Fri 18 Jan 02:31:54 GMT 2019
Angece
[/dev/vdc][/data1-01]
---- ----
hostname [Angece]
devpath  [/dev/vdc]
mntpath  [/data1-01]
---- ----
Found existing filesystem [/dev/vdc]
Creating mount point [/data1-01]
Registering filesystem [/data1-01]
UUID=e96fe7d3-5c72-4c5b-87cb-8ca2626c8bec /data1-01    btrfs    defaults,noatime    0  0
Checking data space [/data1-01]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdc         32G   17M   30G   1% /data1-01

[/dev/vdd][/data2-01]
Fri 18 Jan 02:31:55 GMT 2019
Angece
[/dev/vdd][/data2-01]
---- ----
hostname [Angece]
devpath  [/dev/vdd]
mntpath  [/data2-01]
---- ----
Found existing filesystem [/dev/vdd]
Creating mount point [/data2-01]
Registering filesystem [/data2-01]
UUID=36c83514-e392-4b6f-a13e-6ab4520a8874 /data2-01    btrfs    defaults,noatime    0  0
Checking data space [/data2-01]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdd         32G   31G  4.0M 100% /data2-01

[/dev/vde][/data1-02]
Fri 18 Jan 02:31:56 GMT 2019
Angece
[/dev/vde][/data1-02]
---- ----
hostname [Angece]
devpath  [/dev/vde]
mntpath  [/data1-02]
---- ----
Found existing filesystem [/dev/vde]
Creating mount point [/data1-02]
Registering filesystem [/data1-02]
UUID=c553c0e4-b693-467a-ae4a-c594cc4d7afa /data1-02    btrfs    defaults,noatime    0  0
Checking data space [/data1-02]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vde         64G   51G   12G  82% /data1-02

[/dev/vdf][/data2-02]
Fri 18 Jan 02:31:57 GMT 2019
Angece
[/dev/vdf][/data2-02]
---- ----
hostname [Angece]
devpath  [/dev/vdf]
mntpath  [/data2-02]
---- ----
Found existing filesystem [/dev/vdf]
Creating mount point [/data2-02]
Registering filesystem [/data2-02]
UUID=e3a32ee7-c334-4346-820c-707e46c1d3d8 /data2-02    btrfs    defaults,noatime    0  0
Checking data space [/data2-02]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdf         64G   46G   17G  74% /data2-02

[/dev/vdg][/data1-03]
Fri 18 Jan 02:31:58 GMT 2019
Angece
[/dev/vdg][/data1-03]
---- ----
hostname [Angece]
devpath  [/dev/vdg]
mntpath  [/data1-03]
---- ----
Creating btrfs filesystem [/dev/vdg]
btrfs-progs v4.17.1
See http://btrfs.wiki.kernel.org for more information.

Label:              (null)
UUID:               848db119-1513-452a-bb72-ae6bfaa06cd7
Node size:          16384
Sector size:        4096
Filesystem size:    256.00GiB
Block group profiles:
  Data:             single            8.00MiB
  Metadata:         DUP               1.00GiB
  System:           DUP               8.00MiB
SSD detected:       no
Incompat features:  extref, skinny-metadata
Number of devices:  1
Devices:
   ID        SIZE  PATH
    1   256.00GiB  /dev/vdg

Creating mount point [/data1-03]
Registering filesystem [/data1-03]
UUID=848db119-1513-452a-bb72-ae6bfaa06cd7 /data1-03    btrfs    defaults,noatime    0  0
Checking data space [/data1-03]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdg        256G   17M  254G   1% /data1-03

[/dev/vdh][/data2-03]
Fri 18 Jan 02:31:59 GMT 2019
Angece
[/dev/vdh][/data2-03]
---- ----
hostname [Angece]
devpath  [/dev/vdh]
mntpath  [/data2-03]
---- ----
Creating btrfs filesystem [/dev/vdh]
btrfs-progs v4.17.1
See http://btrfs.wiki.kernel.org for more information.

Label:              (null)
UUID:               06598ab6-fa72-45e1-afe1-04eea5ad08ba
Node size:          16384
Sector size:        4096
Filesystem size:    256.00GiB
Block group profiles:
  Data:             single            8.00MiB
  Metadata:         DUP               1.00GiB
  System:           DUP               8.00MiB
SSD detected:       no
Incompat features:  extref, skinny-metadata
Number of devices:  1
Devices:
   ID        SIZE  PATH
    1   256.00GiB  /dev/vdh

Creating mount point [/data2-03]
Registering filesystem [/data2-03]
UUID=06598ab6-fa72-45e1-afe1-04eea5ad08ba /data2-03    btrfs    defaults,noatime    0  0
Checking data space [/data2-03]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdh        256G   17M  254G   1% /data2-03
-------- -------- -------- --------
Node [Edwalafia]

[/dev/vdc][/data1-01]
Fri 18 Jan 02:32:00 GMT 2019
Edwalafia
[/dev/vdc][/data1-01]
---- ----
hostname [Edwalafia]
devpath  [/dev/vdc]
mntpath  [/data1-01]
---- ----
Found existing filesystem [/dev/vdc]
Creating mount point [/data1-01]
Registering filesystem [/data1-01]
UUID=e6c5141e-f215-4bc8-ac0b-0c93f478fde4 /data1-01    btrfs    defaults,noatime    0  0
Checking data space [/data1-01]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdc         32G   17M   30G   1% /data1-01

[/dev/vdd][/data2-01]
Fri 18 Jan 02:32:01 GMT 2019
Edwalafia
[/dev/vdd][/data2-01]
---- ----
hostname [Edwalafia]
devpath  [/dev/vdd]
mntpath  [/data2-01]
---- ----
Found existing filesystem [/dev/vdd]
Creating mount point [/data2-01]
Registering filesystem [/data2-01]
UUID=4bdd314d-4f4f-4870-8d40-086b0ac13e77 /data2-01    btrfs    defaults,noatime    0  0
Checking data space [/data2-01]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdd         32G   31G  3.5M 100% /data2-01

[/dev/vde][/data1-02]
Fri 18 Jan 02:32:02 GMT 2019
Edwalafia
[/dev/vde][/data1-02]
---- ----
hostname [Edwalafia]
devpath  [/dev/vde]
mntpath  [/data1-02]
---- ----
Found existing filesystem [/dev/vde]
Creating mount point [/data1-02]
Registering filesystem [/data1-02]
UUID=a921ae6f-02b0-4da3-88dd-bf5252cfcfdb /data1-02    btrfs    defaults,noatime    0  0
Checking data space [/data1-02]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vde         64G   48G   15G  77% /data1-02

[/dev/vdf][/data2-02]
Fri 18 Jan 02:32:03 GMT 2019
Edwalafia
[/dev/vdf][/data2-02]
---- ----
hostname [Edwalafia]
devpath  [/dev/vdf]
mntpath  [/data2-02]
---- ----
Found existing filesystem [/dev/vdf]
Creating mount point [/data2-02]
Registering filesystem [/data2-02]
UUID=ce19d30f-2583-4106-b093-e67127beb96d /data2-02    btrfs    defaults,noatime    0  0
Checking data space [/data2-02]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdf         64G   47G   16G  76% /data2-02

[/dev/vdg][/data1-03]
Fri 18 Jan 02:32:04 GMT 2019
Edwalafia
[/dev/vdg][/data1-03]
---- ----
hostname [Edwalafia]
devpath  [/dev/vdg]
mntpath  [/data1-03]
---- ----
Creating btrfs filesystem [/dev/vdg]
btrfs-progs v4.17.1
See http://btrfs.wiki.kernel.org for more information.

Label:              (null)
UUID:               8624f474-900f-4727-8490-537ddc1ae647
Node size:          16384
Sector size:        4096
Filesystem size:    256.00GiB
Block group profiles:
  Data:             single            8.00MiB
  Metadata:         DUP               1.00GiB
  System:           DUP               8.00MiB
SSD detected:       no
Incompat features:  extref, skinny-metadata
Number of devices:  1
Devices:
   ID        SIZE  PATH
    1   256.00GiB  /dev/vdg

Creating mount point [/data1-03]
Registering filesystem [/data1-03]
UUID=8624f474-900f-4727-8490-537ddc1ae647 /data1-03    btrfs    defaults,noatime    0  0
Checking data space [/data1-03]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdg        256G   17M  254G   1% /data1-03

[/dev/vdh][/data2-03]
Fri 18 Jan 02:32:05 GMT 2019
Edwalafia
[/dev/vdh][/data2-03]
---- ----
hostname [Edwalafia]
devpath  [/dev/vdh]
mntpath  [/data2-03]
---- ----
Creating btrfs filesystem [/dev/vdh]
btrfs-progs v4.17.1
See http://btrfs.wiki.kernel.org for more information.

Label:              (null)
UUID:               78faf4a4-b4d4-4a78-a7cf-00eb4bd0a455
Node size:          16384
Sector size:        4096
Filesystem size:    256.00GiB
Block group profiles:
  Data:             single            8.00MiB
  Metadata:         DUP               1.00GiB
  System:           DUP               8.00MiB
SSD detected:       no
Incompat features:  extref, skinny-metadata
Number of devices:  1
Devices:
   ID        SIZE  PATH
    1   256.00GiB  /dev/vdh

Creating mount point [/data2-03]
Registering filesystem [/data2-03]
UUID=78faf4a4-b4d4-4a78-a7cf-00eb4bd0a455 /data2-03    btrfs    defaults,noatime    0  0
Checking data space [/data2-03]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdh        256G   17M  254G   1% /data2-03
-------- -------- -------- --------
Node [Onoza]

[/dev/vdc][/data1-01]
Fri 18 Jan 02:32:06 GMT 2019
Onoza
[/dev/vdc][/data1-01]
---- ----
hostname [Onoza]
devpath  [/dev/vdc]
mntpath  [/data1-01]
---- ----
Found existing filesystem [/dev/vdc]
Creating mount point [/data1-01]
Registering filesystem [/data1-01]
UUID=f26ec954-ee3c-40c2-9d9c-2f31c9eca4c6 /data1-01    btrfs    defaults,noatime    0  0
Checking data space [/data1-01]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdc         32G   17M   30G   1% /data1-01

[/dev/vdd][/data2-01]
Fri 18 Jan 02:32:07 GMT 2019
Onoza
[/dev/vdd][/data2-01]
---- ----
hostname [Onoza]
devpath  [/dev/vdd]
mntpath  [/data2-01]
---- ----
Found existing filesystem [/dev/vdd]
Creating mount point [/data2-01]
Registering filesystem [/data2-01]
UUID=26fda47f-cf3b-47af-a23e-80c4f911ee9b /data2-01    btrfs    defaults,noatime    0  0
Checking data space [/data2-01]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdd         32G   31G  4.6M 100% /data2-01

[/dev/vde][/data1-02]
Fri 18 Jan 02:32:08 GMT 2019
Onoza
[/dev/vde][/data1-02]
---- ----
hostname [Onoza]
devpath  [/dev/vde]
mntpath  [/data1-02]
---- ----
Found existing filesystem [/dev/vde]
Creating mount point [/data1-02]
Registering filesystem [/data1-02]
UUID=dde98c24-9112-45b4-8ca6-6d6ac85ca9b0 /data1-02    btrfs    defaults,noatime    0  0
Checking data space [/data1-02]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vde         64G   46G   17G  74% /data1-02

[/dev/vdf][/data2-02]
Fri 18 Jan 02:32:09 GMT 2019
Onoza
[/dev/vdf][/data2-02]
---- ----
hostname [Onoza]
devpath  [/dev/vdf]
mntpath  [/data2-02]
---- ----
Found existing filesystem [/dev/vdf]
Creating mount point [/data2-02]
Registering filesystem [/data2-02]
UUID=ceba1807-350d-44be-9640-b01c57bdc749 /data2-02    btrfs    defaults,noatime    0  0
Checking data space [/data2-02]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdf         64G   49G   14G  79% /data2-02

[/dev/vdg][/data1-03]
Fri 18 Jan 02:32:10 GMT 2019
Onoza
[/dev/vdg][/data1-03]
---- ----
hostname [Onoza]
devpath  [/dev/vdg]
mntpath  [/data1-03]
---- ----
Creating btrfs filesystem [/dev/vdg]
btrfs-progs v4.17.1
See http://btrfs.wiki.kernel.org for more information.

Label:              (null)
UUID:               f7fc3c42-4187-46f7-93f0-4ca023c20a5b
Node size:          16384
Sector size:        4096
Filesystem size:    256.00GiB
Block group profiles:
  Data:             single            8.00MiB
  Metadata:         DUP               1.00GiB
  System:           DUP               8.00MiB
SSD detected:       no
Incompat features:  extref, skinny-metadata
Number of devices:  1
Devices:
   ID        SIZE  PATH
    1   256.00GiB  /dev/vdg

Creating mount point [/data1-03]
Registering filesystem [/data1-03]
UUID=f7fc3c42-4187-46f7-93f0-4ca023c20a5b /data1-03    btrfs    defaults,noatime    0  0
Checking data space [/data1-03]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdg        256G   17M  254G   1% /data1-03

[/dev/vdh][/data2-03]
Fri 18 Jan 02:32:11 GMT 2019
Onoza
[/dev/vdh][/data2-03]
---- ----
hostname [Onoza]
devpath  [/dev/vdh]
mntpath  [/data2-03]
---- ----
Creating btrfs filesystem [/dev/vdh]
btrfs-progs v4.17.1
See http://btrfs.wiki.kernel.org for more information.

Label:              (null)
UUID:               384f0ffe-8dc1-4374-83c2-ba287d5f286e
Node size:          16384
Sector size:        4096
Filesystem size:    256.00GiB
Block group profiles:
  Data:             single            8.00MiB
  Metadata:         DUP               1.00GiB
  System:           DUP               8.00MiB
SSD detected:       no
Incompat features:  extref, skinny-metadata
Number of devices:  1
Devices:
   ID        SIZE  PATH
    1   256.00GiB  /dev/vdh

Creating mount point [/data2-03]
Registering filesystem [/data2-03]
UUID=384f0ffe-8dc1-4374-83c2-ba287d5f286e /data2-03    btrfs    defaults,noatime    0  0
Checking data space [/data2-03]
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdh        256G   17M  254G   1% /data2-03
--END--

    # First attempt accidentally wiped /dev/vdc on each node.
    # Bad  - we lost 32G of data on each node.
    # Good - we didn't loose the data on the other disks
    # Check before you create the filesystem.
    # Interesting to see how robust Kafka is ...
    # How much did we loose ?






